services:
  # PostgreSQL Database
  db:
    image: postgres:15-alpine
    container_name: content-crew-db
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-contentcrew}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-contentcrew123}
      POSTGRES_DB: ${POSTGRES_DB:-content_crew}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-contentcrew}"]
      interval: 10s
      timeout: 2s
      retries: 3
      start_period: 10s
    networks:
      - content-crew-network
    restart: unless-stopped

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: content-crew-redis
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 1s
      retries: 3
      start_period: 5s
    networks:
      - content-crew-network
    restart: unless-stopped

  # Backend API Service
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: content-crew-api
    # Load environment variables from .env file
    # Create .env from .env.example: cp .env.example .env
    env_file:
      - .env
    environment:
      # Environment
      ENV: ${ENV:-dev}
      
      # Database
      DATABASE_URL: postgresql://${POSTGRES_USER:-contentcrew}:${POSTGRES_PASSWORD:-contentcrew123}@db:5432/${POSTGRES_DB:-content_crew}
      
      # Redis
      REDIS_URL: ${REDIS_URL:-redis://redis:6379/0}
      
      # Security
      # SECRET_KEY is REQUIRED - no default fallback for security
      # Set it in .env file or via environment variable
      # Generate with: openssl rand -hex 32
      # Note: Docker Compose doesn't support :? syntax, so we use empty default
      # The application will validate and fail if SECRET_KEY is not set
      SECRET_KEY: ${SECRET_KEY:-}
      
      # OAuth (optional - set in .env)
      GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID:-}
      GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET:-}
      FACEBOOK_CLIENT_ID: ${FACEBOOK_CLIENT_ID:-}
      FACEBOOK_CLIENT_SECRET: ${FACEBOOK_CLIENT_SECRET:-}
      GITHUB_CLIENT_ID: ${GITHUB_CLIENT_ID:-}
      GITHUB_CLIENT_SECRET: ${GITHUB_CLIENT_SECRET:-}
      
      # Payment providers (optional - set in .env)
      STRIPE_SECRET_KEY: ${STRIPE_SECRET_KEY:-}
      STRIPE_WEBHOOK_SECRET: ${STRIPE_WEBHOOK_SECRET:-}
      PAYSTACK_SECRET_KEY: ${PAYSTACK_SECRET_KEY:-}
      PAYSTACK_WEBHOOK_SECRET: ${PAYSTACK_WEBHOOK_SECRET:-}
      
      # Frontend callback URLs
      FRONTEND_CALLBACK_URL: ${FRONTEND_CALLBACK_URL:-http://localhost:3000/auth/callback}
      API_BASE_URL: ${API_BASE_URL:-http://localhost:8000}
      
      # LLM Provider Configuration
      # OpenAI API Key (required if using OpenAI models like gpt-4o-mini)
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      # Ollama connection (optional - only needed if using Ollama models)
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}
      
      # Build metadata
      BUILD_VERSION: ${BUILD_VERSION:-dev}
      COMMIT_HASH: ${COMMIT_HASH:-unknown}
      BUILD_TIME: ${BUILD_TIME:-unknown}
      
      # Python
      PYTHONUNBUFFERED: 1
      PORT: 8000
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    # Note: Ollama dependency removed - use external Ollama or enable via profile
    healthcheck:
      test: ["CMD", "curl", "-f", "--max-time", "3", "http://localhost:8000/health"]
      interval: 15s
      timeout: 3s
      retries: 3
      start_period: 30s
    networks:
      - content-crew-network
    restart: unless-stopped

  # Frontend Web UI
  web:
    build:
      context: ./web-ui
      dockerfile: Dockerfile
      args:
        NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://localhost:8000}
    container_name: content-crew-web
    # Load environment variables from .env file (for NEXT_PUBLIC_* vars)
    env_file:
      - .env
    environment:
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://localhost:8000}
      NEXT_PUBLIC_GOOGLE_CLIENT_ID: ${NEXT_PUBLIC_GOOGLE_CLIENT_ID:-}
      NEXT_PUBLIC_FACEBOOK_CLIENT_ID: ${NEXT_PUBLIC_FACEBOOK_CLIENT_ID:-}
      NEXT_PUBLIC_GITHUB_CLIENT_ID: ${NEXT_PUBLIC_GITHUB_CLIENT_ID:-}
      NODE_ENV: production
    ports:
      - "3000:3000"
    depends_on:
      api:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - content-crew-network
    restart: unless-stopped

  # Ollama Service (optional - use profile to enable)
  ollama:
    image: ollama/ollama:latest
    container_name: content-crew-ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    networks:
      - content-crew-network
    restart: unless-stopped
    profiles:
      - ollama
    # Model Download:
    # After starting Ollama, download required models:
    #   Option 1 (Recommended): make models-pull
    #   Option 2: docker compose exec ollama ollama pull llama3.2:1b
    #   Option 3: Use infra/scripts/pull-models.sh or pull-models.py
    #
    # Required models (per tier):
    #   - Free: llama3.2:1b (~1.3 GB)
    #   - Basic: llama3.2:3b (~2.0 GB)
    #   - Pro: llama3.1:8b (~4.7 GB)
    #   - Enterprise: llama3.1:70b (~40 GB)
    #
    # See docs/models.md for complete model management guide

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  ollama_data:
    driver: local

networks:
  content-crew-network:
    driver: bridge

